# -*- coding: utf-8 -*-
"""D4RL_install_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10hIxZFoZ7xDA5Jh-Gz9n_DUx9-wylYn5
"""

###### libs for install ######


!sudo apt-get update
!sudo apt-get install gcc

!sudo apt-get build-dep mesa
!sudo apt-get install llvm-dev
!sudo apt-get install freeglut3 freeglut3-dev

!sudo apt-get install python3-dev

!sudo apt-get install build-essential

!sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \
        libosmesa6-dev software-properties-common net-tools unzip vim \
        virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf

#!sudo apt-get install -y libglew-dev

###### mujoco setup ######


#!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz

!wget https://roboti.us/download/mujoco200_linux.zip

!wget https://roboti.us/file/mjkey.txt

!mkdir /root/.mujoco

### mujoco 210
#!tar -xf mujoco210-linux-x86_64.tar.gz -C /.mujoco/
#!ls -alh /.mujoco/mujoco210

### mujoco 200
!unzip mujoco200_linux.zip -d /root/.mujoco/
!cp -r /root/.mujoco/mujoco200_linux /root/.mujoco/mujoco200

!mv mjkey.txt /root/.mujoco/

!cp -r /root/.mujoco/mujoco200/bin/* /usr/lib/

!ls -alh /root/.mujoco/

# Commented out IPython magic to ensure Python compatibility.

# %env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin

###### mujoco-py setup ######
!pip install "cython<3"
!pip install mujoco_py==2.0.2.8

###### D4RL setup ######

## !pip uninstall dm_control==0.0.364896371

!git clone https://github.com/rail-berkeley/d4rl.git

### edit dm_control version in d4rl setup.py
!sed -i "s;dm_control @ git+git://github.com/deepmind/dm_control@master#egg=dm_control;dm_control==0.0.364896371;g" /content/d4rl/setup.py

### edit mjrl install in d4rl setup.py to use github's new https protocol instead of git SSH
!sed -i "s;mjrl @ git+git://github.com/aravindr93/mjrl@master#egg=mjrl;mjrl @ git+https://github.com/aravindr93/mjrl@master#egg=mjrl;g" /content/d4rl/setup.py

!pip install -e d4rl/.

###### restart runtime ######

exit()

# Commented out IPython magic to ensure Python compatibility.
# set mujoco env path if not already set
# %env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin

import gym
import d4rl # Import required to register environments


env = gym.make('Walker2d-v3')
env.reset()
env.step(env.action_space.sample())
env.close()
print("mujoco-py check passed")

env = gym.make('walker2d-medium-v2')
env.reset()
env.step(env.action_space.sample())
env.close()
print("d4rl check passed")

# Commented out IPython magic to ensure Python compatibility.

# set mujoco env path if not already set
# %env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin

import os
import gym
import numpy as np

import collections
import pickle

import d4rl

datasets = []

data_dir = "./data"

print(data_dir)

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

for env_name in ['walker2d', 'halfcheetah', 'hopper']:
    for dataset_type in ['medium', 'medium-expert', 'medium-replay']:

        name = f'{env_name}-{dataset_type}-v2'
        pkl_file_path = os.path.join(data_dir, name)

        print("processing: ", name)

        env = gym.make(name)
        dataset = env.get_dataset()

        N = dataset['rewards'].shape[0]
        data_ = collections.defaultdict(list)

        use_timeouts = False
        if 'timeouts' in dataset:
            use_timeouts = True

        episode_step = 0
        paths = []
        for i in range(N):
            done_bool = bool(dataset['terminals'][i])
            if use_timeouts:
                final_timestep = dataset['timeouts'][i]
            else:
                final_timestep = (episode_step == 1000-1)
            for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
                data_[k].append(dataset[k][i])
            if done_bool or final_timestep:
                episode_step = 0
                episode_data = {}
                for k in data_:
                    episode_data[k] = np.array(data_[k])
                paths.append(episode_data)
                data_ = collections.defaultdict(list)
            episode_step += 1

        returns = np.array([np.sum(p['rewards']) for p in paths])
        num_samples = np.sum([p['rewards'].shape[0] for p in paths])
        print(f'Number of samples collected: {num_samples}')
        print(f'Trajectory returns: mean = {np.mean(returns)}, std = {np.std(returns)}, max = {np.max(returns)}, min = {np.min(returns)}')

        with open(f'{pkl_file_path}.pkl', 'wb') as f:
            pickle.dump(paths, f)

import gym
import d4rl # Import required to register environments, you may need to also import the submodulept

# Create the environmentexit
env = gym.make('maze2d-umaze-v1')

# d4rl abides by the OpenAI gym interface
env.reset()
env.step(env.action_space.sample())

# Each task is associated with a dataset
# dataset contains observations, actions, rewards, terminals, and infos
dataset = env.get_dataset()
print(dataset['observations']) # An N x dim_observation Numpy array of observations

# Alternatively, use d4rl.qlearning_dataset which
# also adds next_observations.
dataset = d4rl.qlearning_dataset(env)

import gym
import d4rl # Import required to register environments, you may need to also import the submodulept

# Create the environmentexit
env = gym.make('walker2d-medium')

# d4rl abides by the OpenAI gym interface
env.reset()
env.step(env.action_space.sample())

# Each task is associated with a dataset
# dataset contains observations, actions, rewards, terminals, and infos
dataset = env.get_dataset()
print(dataset['observations']) # An N x dim_observation Numpy array of observations

# Alternatively, use d4rl.qlearning_dataset which
# also adds next_observations.
dataset = d4rl.qlearning_dataset(env)
env.render()

import gym
import d4rl

env = gym.make('maze2d-umaze-v1')
env.reset()
dataset = env.get_dataset()

print("Observation Shape:", dataset['observations'].shape)
print("Action Shape:", dataset['actions'].shape)
print("Reward Shape:", dataset['rewards'].shape)
print("Terminal Shape:", dataset['terminals'].shape)

for _ in range(10):
    action = env.action_space.sample()
    print("Action at this time:", action)
    observation, reward, done, info = env.step(action)
    print("Observation:", observation)
    print("Reward:", reward)
    print("Done:", done)
    dataset = d4rl.qlearning_dataset(env)
    if done:
        observation = env.reset()

import gym
import d4rl

env = gym.make('walker2d-medium-v2')
env.reset()
dataset = env.get_dataset()

print("Observation Shape:", dataset['observations'].shape)
print("Action Shape:", dataset['actions'].shape)
print("Reward Shape:", dataset['rewards'].shape)
print("Terminal Shape:", dataset['terminals'].shape)

for _ in range(10):
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
    print("Observation:", observation)
    print("Reward:", reward)
    print("Done:", done)
    dataset = d4rl.qlearning_dataset(env)
    if done:
        observation = env.reset()

import gym
import d4rl
from pyvirtualdisplay import Display
from IPython import display as ipythondisplay
import matplotlib.pyplot as plt

display = Display(visible=0, size=(1400, 900))
display.start()

env = gym.make('walker2d-medium-v2')
dataset = env.get_dataset()

for _ in range(10):
    observation = env.reset()
    done = False
    while not done:
        action = env.action_space.sample()
        observation, reward, done, _ = env.step(action)

        plt.imshow(env.render(mode='rgb_array'))
        plt.axis('off')
        plt.show()

import gym
import d4rl

# Register the environment
import d4rl  # Import required to register environments, you may need to also import the submodule
import numpy as np

# Create the environment
env = gym.make('maze2d-umaze-v1')

# Make sure the environment is successfully created
assert env is not None, "Environment not successfully created"

# Reset the environment
obs = env.reset()
print("Initial observation size:", obs.shape)

# Take a random action and execute it
action = env.action_space.sample()
next_obs, reward, done, info = env.step(action)
print("Observation size after action:", next_obs.shape)
print("Reward:", reward)
print("Done:", done)
print("Info:", info)

# Get the dataset
dataset = env.get_dataset()
print("Dataset observation size:", dataset['observations'].shape)
print("Dataset action size:", dataset['actions'].shape)
print("Dataset reward size:", dataset['rewards'].shape)
print("Dataset terminal size:", dataset['terminals'].shape)
#print("Dataset info size:", len(dataset['infos']))

# Use d4rl.qlearning_dataset to get the dataset with next observations
q_dataset = d4rl.qlearning_dataset(env)
print("Q-learning dataset observation size:", q_dataset['observations'].shape)
print("Q-learning dataset action size:", q_dataset['actions'].shape)
print("Q-learning dataset reward size:", q_dataset['rewards'].shape)
print("Q-learning dataset terminal size:", q_dataset['terminals'].shape)
print("Q-learning dataset next observation size:", q_dataset['next_observations'].shape)

