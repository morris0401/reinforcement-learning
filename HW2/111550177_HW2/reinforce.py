# -*- coding: utf-8 -*-
"""reinforce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FxnfF0goT_OS0J3v0Ziswp-Hup9H2tfG
"""

# Commented out IPython magic to ensure Python compatibility.
# Spring 2024, 535514 Reinforcement Learning
# HW2: REINFORCE and baseline
# %load_ext tensorboard
# %tensorboard --logdir=./tb_record_1
import os
import gym
from itertools import count
from collections import namedtuple
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
import torch.optim.lr_scheduler as Scheduler
from torch.utils.tensorboard import SummaryWriter

# Define a useful tuple (optional)
SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])

# Define a tensorboard writer
writer = SummaryWriter("./tb_record_1")

class Policy(nn.Module):
    """
        Implement both policy network and the value network in one model
        - Note that here we let the actor and value networks share the first layer
        - Feel free to change the architecture (e.g. number of hidden layers and the width of each hidden layer) as you like
        - Feel free to add any member variables/functions whenever needed
        TODO:
            1. Initialize the network (including the GAE parameters, shared layer(s), the action layer(s), and the value layer(s))
            2. Random weight initialization of each layer
    """
    def __init__(self):
        super(Policy, self).__init__()

        # Extract the dimensionality of state and action spaces
        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)
        self.observation_dim = env.observation_space.shape[0]
        self.action_dim = env.action_space.n if self.discrete else env.action_space.shape[0]
        self.hidden_size = 256
        self.double()

        ########## YOUR CODE HERE (5~10 lines) ##########
        self.shared_layer1 = nn.Linear(self.observation_dim, self.hidden_size)  # Shared layer 1
        self.shared_layer2 = nn.Linear(self.hidden_size, self.hidden_size)  # Shared layer 2
        self.dropout = nn.Dropout(0.2)  # Dropout layer
        self.action_layer = nn.Linear(self.hidden_size, self.action_dim)  # Action layer
        self.value_layer = nn.Linear(self.hidden_size, 1)  # Value layer

        ########## END OF YOUR CODE ##########

        # action & reward memory
        self.saved_actions = []
        self.rewards = []

    def forward(self, state):
        """
            Forward pass of both policy and value networks
            - The input is the state, and the outputs are the corresponding
              action probability distirbution and the state value
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        x = self.shared_layer1(state)  # Forward pass through shared layer 1
        x = F.relu(x)  # Apply ReLU activation function
        x = self.dropout(x)  # Apply dropout
        x = self.shared_layer2(x)  # Forward pass through shared layer 2
        x = F.relu(x)  # Apply ReLU activation function
        action_prob = self.action_layer(x)  # Action probabilities
        state_value = self.value_layer(x)  # State value estimation

        ########## END OF YOUR CODE ##########

        return action_prob, state_value


    def select_action(self, state):
        """
            Select the action given the current state
            - The input is the state, and the output is the action to apply
            (based on the learned stochastic policy)
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        #state = torch.Tensor(state)  # Convert state to PyTorch tensor
        action, state_value = self.forward(torch.Tensor(state))  # Forward pass through the network
        m = Categorical(logits=action)  # Categorical distribution based on action probabilities
        action = m.sample()  # Sample an action from the distribution
        ########## END OF YOUR CODE ##########

        # save to action buffer
        self.saved_actions.append(SavedAction(m.log_prob(action), state_value))

        return action.item()


    def calculate_loss(self, gamma=0.999):
        """
            Calculate the loss (= policy loss + value loss) to perform backprop later
            TODO:
                1. Calculate rewards-to-go required by REINFORCE with the help of self.rewards
                2. Calculate the policy loss using the policy gradient
                3. Calculate the value loss using either MSE loss or smooth L1 loss
        """

        # Initialize the lists and variables
        R = 0
        saved_actions = self.saved_actions
        policy_losses = []
        value_losses = []
        returns = []

        ########## YOUR CODE HERE (8-15 lines) ##########
        discounted_sum = 0  # Initialize discounted sum
        #for reward in reversed(self.rewards):  # Iterate over rewards in reverse order
        #    discounted_sum = reward + gamma * discounted_sum  # Calculate discounted sum
        #    returns.append(discounted_sum)  # Append to returns
        # Convert rewards to numpy array
        rewards_np = np.array(self.rewards)

        # Calculate discounts
        #discounts = np.array([gamma ** i for i in range(len(rewards_np))], dtype=np.float32)
        discounts = gamma ** np.arange(len(rewards_np))

        # Reverse the rewards array
        rewards_reverse = rewards_np[::-1]

        # Compute discounted sums using cumulative sum and element-wise multiplication
        discounted_sums = np.cumsum(rewards_reverse * discounts[::-1])[::-1]

        # Append discounted sums to returns
        #returns = discounted_sums.tolist()
        #returns.reverse()  # Reverse the list to align with chronological order

        returns = torch.Tensor(discounted_sums.tolist())  # Convert to PyTorch tensor
        returns = (returns - returns.mean()) / (returns.std() + 0.001)  # Normalize returns
        action_log_probs = torch.stack([action.log_prob for action in saved_actions], dim=0)  # Stack log probabilities
        loss = -(returns * action_log_probs).sum()  # Policy loss calculation

        value_losses = F.mse_loss(torch.stack([action.value for action  in saved_actions], dim=0)[:,0], returns)
        loss += value_losses  # Value loss calculation

        ########## END OF YOUR CODE ##########

        return loss

    def clear_memory(self):
        # reset rewards and action buffer
        del self.rewards[:]
        del self.saved_actions[:]

class GAE:
    def __init__(self, gamma, lambda_, num_steps):
        self.gamma = gamma
        self.lambda_ = lambda_
        self.num_steps = num_steps          # set num_steps = None to adapt full batch

    def __call__(self, rewards, values, done):
      """
          Implement Generalized Advantage Estimation (GAE) for your value prediction
          TODO (1): Pass correct corresponding inputs (rewards, values, and done) into the function arguments
          TODO (2): Calculate the Generalized Advantage Estimation and return the obtained value
      """

        ########## YOUR CODE HERE (8-15 lines) ##########




        ########## END OF YOUR CODE ##########

def train(lr=0.01):
    """
        Train the model using SGD (via backpropagation)
        TODO (1): In each episode,
        1. run the policy till the end of the episode and keep the sampled trajectory
        2. update both the policy and the value network at the end of episode

        TODO (2): In each episode,
        1. record all the value you aim to visualize on tensorboard (lr, reward, length, ...)
    """

    # Instantiate the policy model and the optimizer
    model = Policy()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Learning rate scheduler (optional)
    # scheduler = Scheduler.StepLR(optimizer, step_size=100, gamma=0.9)

    # EWMA reward for tracking the learning progress
    ewma_reward = 0

    # run inifinitely many episodes
    for i_episode in count(1):
        # reset environment and episode reward
        state = env.reset()
        ep_reward = 0
        t = 0

        # Uncomment the following line to use learning rate scheduler
        # scheduler.step()

        # For each episode, only run 9999 steps to avoid entering infinite loop during the learning process

        ########## YOUR CODE HERE (10-15 lines) ##########
        while True:
            t += 1  # Increment time step
            action = model.select_action(state)  # Select action based on current state
            state, reward, done, _ = env.step(action)  # Take action in the environment

            ep_reward += reward  # Update episode reward
            model.rewards.append(reward)  # Append reward to rewards buffer

            if done:  # If episode is done
                break

        optimizer.zero_grad()  # Zero gradients
        loss = model.calculate_loss(0.99999)  # Calculate loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Optimizer step
        model.clear_memory()  # Clear memory
        ########## END OF YOUR CODE ##########

        # update EWMA reward and log the results
        ewma_reward = 0.05 * ep_reward + (1 - 0.05) * ewma_reward
        print('Episode {}\tlength: {}\treward: {}\t ewma reward: {}'.format(i_episode, t, ep_reward, ewma_reward))

        #Try to use Tensorboard to record the behavior of your implementation
        ########## YOUR CODE HERE (4-5 lines) ##########
        writer.add_scalar('Training Loss', loss, i_episode)  # Record training loss
        writer.add_scalar('EWMA reward', ewma_reward, i_episode)  # Record EWMA reward
        writer.add_scalar('Episode reward', ep_reward, i_episode)  # Record episode reward
        writer.add_scalar('Length', t, i_episode)  # Record episode len
        #writer.add_scalar('Learning Rate', scheduler.get_lr()[-1], i_episode)  # Record learning rate
        ########## END OF YOUR CODE ##########

        # check if we have "solved" the cart pole problem, use 120 as the threshold in LunarLander-v2
        if ewma_reward > env.spec.reward_threshold:
            if not os.path.isdir("./preTrained"):
                os.mkdir("./preTrained")
            torch.save(model.state_dict(), './preTrained/CartPole_{}.pth'.format(lr))
            print("Solved! Running reward is now {} and "
                  "the last episode runs to {} time steps!".format(ewma_reward, t))
            break


def test(name, n_episodes=10):
    """
        Test the learned model (no change needed)
    """
    model = Policy()

    model.load_state_dict(torch.load('./preTrained/{}'.format(name)))

    render = True
    max_episode_len = 10000

    for i_episode in range(1, n_episodes+1):
        state = env.reset()
        running_reward = 0
        for t in range(max_episode_len+1):
            action = model.select_action(state)
            state, reward, done, _ = env.step(action)
            running_reward += reward
            if render:
                 env.render()
            if done:
                break
        print('Episode {}\tReward: {}'.format(i_episode, running_reward))
    env.close()


if __name__ == '__main__':
    # For reproducibility, fix the random seed
    random_seed = 10
    lr = 0.001
    env = gym.make('CartPole-v0')
    env.seed(random_seed)
    torch.manual_seed(random_seed)
    train(lr)
    test(f'CartPole_{lr}.pth')

# Import necessary libraries
import os  # Operating system library for file operations
import gym  # OpenAI Gym library for RL environments
from itertools import count  # Iterator tools for counting
from collections import namedtuple  # Collections library for named tuples
import numpy as np  # Numerical computing library

import torch  # PyTorch deep learning library
import torch.nn as nn  # Neural network module in PyTorch
import torch.nn.functional as F  # Functional module in PyTorch
import torch.optim as optim  # Optimization module in PyTorch
from torch.distributions import Categorical  # Probability distributions in PyTorch
import torch.optim.lr_scheduler as Scheduler  # Learning rate scheduler in PyTorch
from torch.utils.tensorboard import SummaryWriter  # TensorBoard integration for PyTorch

# Define a useful tuple (optional)
SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])

# Define a tensorboard writer
writer = SummaryWriter("./tb_record_1/Vanilla")  # Summary writer for TensorBoard

class Policy(nn.Module):
    """
        Implement both policy network and the value network in one model
        - Note that here we let the actor and value networks share the first layer
        - Feel free to change the architecture (e.g. number of hidden layers and the width of each hidden layer) as you like
        - Feel free to add any member variables/functions whenever needed
        TODO:
            1. Initialize the network (including the GAE parameters, shared layer(s), the action layer(s), and the value layer(s))
            2. Random weight initialization of each layer
    """
    def __init__(self):
        super(Policy, self).__init__()

        # Extract the dimensionality of state and action spaces
        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)  # Check if action space is discrete
        self.observation_dim = env.observation_space.shape[0]  # Dimensionality of the observation space
        self.action_dim = env.action_space.n if self.discrete else env.action_space.shape[0]  # Dimensionality of the action space
        self.hidden_size = 128  # Size of the hidden layer
        self.double()  # Convert to double precision floating point

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define neural network layers
        self.shared_layer1 = nn.Linear(self.observation_dim, self.hidden_size)  # Shared layer 1
        self.shared_layer2 = nn.Linear(self.hidden_size, self.hidden_size)  # Shared layer 2
        self.action_layer = nn.Linear(self.hidden_size, self.action_dim)  # Action layer
        self.value_layer = nn.Linear(self.hidden_size, 1)  # Value layer
        ########## END OF YOUR CODE ##########

        # Action & reward memory
        self.saved_actions = []  # Stores the log probabilities and values of selected actions
        self.rewards = []  # Stores rewards

    def forward(self, state):
        """
            Forward pass of both policy and value networks
            - The input is the state, and the outputs are the corresponding
              action probability distribution and the state value
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        x = self.shared_layer1(state)  # Forward pass through shared layer 1
        x = F.relu(x)  # Apply ReLU activation function
        x = self.shared_layer2(x)  # Forward pass through shared layer 2
        x = F.relu(x)  # Apply ReLU activation function
        action_prob = self.action_layer(x)  # Action probabilities
        state_value = self.value_layer(x)  # State value estimation
        ########## END OF YOUR CODE ##########

        return action_prob, state_value

    def select_action(self, state):
        """
            Select the action given the current state
            - The input is the state, and the output is the action to apply
            (based on the learned stochastic policy)
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        state = torch.Tensor(state)  # Convert state to PyTorch tensor
        action, state_value = self.forward(state)  # Forward pass through the network
        m = Categorical(logits=action)  # Categorical distribution based on action probabilities
        action = m.sample()  # Sample an action from the distribution
        ########## END OF YOUR CODE ##########

        # Save action and log probability to action buffer
        self.saved_actions.append(SavedAction(m.log_prob(action), state_value))

        return action.item()  # Return the sampled action

    def calculate_loss(self, gamma=0.999):
        """
            Calculate the loss (= policy loss + value loss) to perform backprop later
            TODO:
                1. Calculate rewards-to-go required by REINFORCE with the help of self.rewards
                2. Calculate the policy loss using the policy gradient
                3. Calculate the value loss using either MSE loss or smooth L1 loss
        """

        # Initialize the lists and variables
        R = 0  # Initialize the rewards-to-go
        saved_actions = self.saved_actions  # Saved actions and log probabilities
        policy_losses = []  # Policy loss
        value_losses = []  # Value loss
        returns = []  # Returns (rewards-to-go)

        ########## YOUR CODE HERE (8-15 lines) ##########
        discounted_sum = 0  # Initialize discounted sum
        for reward in reversed(self.rewards):  # Iterate over rewards in reverse order
            discounted_sum = reward + gamma * discounted_sum  # Calculate discounted sum
            returns.append(discounted_sum)  # Append to returns
        returns.reverse()  # Reverse the list to align with chronological order

        returns = torch.Tensor(returns)  # Convert to PyTorch tensor
        log_probs = [action.log_prob for action in saved_actions]  # Log probabilities of actions
        action_log_probs = torch.stack(log_probs, dim=0)  # Stack log probabilities
        policy_losses = -(returns * action_log_probs).sum()  # Policy loss calculation
        ########## END OF YOUR CODE ##########

        return policy_losses  # Return the policy loss

    def clear_memory(self):
        # Reset rewards and action buffer
        del self.rewards[:]  # Clear rewards
        del self.saved_actions[:]  # Clear saved actions

class GAE:
    def __init__(self, gamma, lambda_, num_steps):
        self.gamma = gamma  # Discount factor
        self.lambda_ = lambda_  # Lambda parameter for GAE
        self.num_steps = num_steps  # Number of steps for GAE calculation
        # Set num_steps = None to adapt full batch

    def __call__(self, rewards, values, done):
        pass
        """
            Implement Generalized Advantage Estimation (GAE) for your value prediction
            TODO (1): Pass correct corresponding inputs (rewards, values, and done) into the function arguments
            TODO (2): Calculate the Generalized Advantage Estimation and return the obtained value
        """

        ########## YOUR CODE HERE (8-15 lines) ##########




        ########## END OF YOUR CODE ##########

def train(lr=0.01):
    """
        Train the model using SGD (via backpropagation)
        TODO (1): In each episode,
        1. run the policy till the end of the episode and keep the sampled trajectory
        2. update both the policy and the value network at the end of episode

        TODO (2): In each episode,
        1. record all the value you aim to visualize on tensorboard (lr, reward, length, ...)
    """

    # Instantiate the policy model and the optimizer
    model = Policy()  # Create policy model
    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer

    # Learning rate scheduler (optional)
    scheduler = Scheduler.StepLR(optimizer, step_size=100, gamma=0.9)

    # EWMA reward for tracking the learning progress
    ewma_reward = 0  # Exponentially Weighted Moving Average reward

    # Run infinitely many episodes
    for i_episode in count(1):
        # Reset environment and episode reward
        state = env.reset()  # Reset environment
        ep_reward = 0  # Initialize episode reward
        t = 0  # Initialize time step

        # Uncomment the following line to use learning rate scheduler
        # scheduler.step()

        # For each episode, only run 9999 steps to avoid entering infinite loop during the learning process

        ########## YOUR CODE HERE (10-15 lines) ##########
        while True:
            t += 1  # Increment time step
            action = model.select_action(state)  # Select action based on current state
            state, reward, done, _ = env.step(action)  # Take action in the environment

            ep_reward += reward  # Update episode reward
            model.rewards.append(reward)  # Append reward to rewards buffer

            if done:  # If episode is done
                break

        optimizer.zero_grad()  # Zero gradients
        loss = model.calculate_loss()  # Calculate loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Optimizer step
        scheduler.step()  # Learning rate scheduler step

        model.clear_memory()  # Clear memory
        ########## END OF YOUR CODE ##########

        # Update EWMA reward and log the results
        ewma_reward = 0.05 * ep_reward + (1 - 0.05) * ewma_reward  # Update EWMA reward
        print('Episode {}\tlength: {}\treward: {}\t ewma reward: {}'.format(i_episode, t, ep_reward, ewma_reward))

        # Try to use Tensorboard to record the behavior of your implementation
        ########## YOUR CODE HERE (4-5 lines) ##########
        writer.add_scalar('training loss', loss, i_episode)  # Record training loss
        writer.add_scalar('EWMA reward', ewma_reward, i_episode)  # Record EWMA reward
        writer.add_scalar('Episode reward', ep_reward, i_episode)  # Record episode reward
        writer.add_scalar('Length', t, i_episode)  # Record episode length
        writer.add_scalar('Learning Rate', scheduler.get_lr()[-1], i_episode)  # Record learning rate
        ########## END OF YOUR CODE ##########

        # Check if we have "solved" the cart pole problem, use 120 as the threshold in LunarLander-v2
        if ewma_reward > env.spec.reward_threshold:
            if not os.path.isdir("./preTrained"):  # If directory does not exist
                os.mkdir("./preTrained")  # Create directory
            torch.save(model.state_dict(), './preTrained/CartPole_{}.pth'.format(lr))  # Save model state
            print("Solved! Running reward is now {} and "
                  "the last episode runs to {} time steps!".format(ewma_reward, t))  # Print message
            break  # Break out of loop


def test(name, n_episodes=10):
    """
        Test the learned model (no change needed)
    """
    model = Policy()  # Create policy model

    model.load_state_dict(torch.load('./preTrained/{}'.format(name)))  # Load model parameters

    render = True  # Rendering flag
    max_episode_len = 10000  # Maximum episode length

    for i_episode in range(1, n_episodes+1):
        state = env.reset()  # Reset environment
        running_reward = 0  # Initialize running reward
        for t in range(max_episode_len+1):
            action = model.select_action(state)  # Select action based on current state
            state, reward, done, _ = env.step(action)  # Take action in the environment
            running_reward += reward  # Update running reward
            if render:
                 env.render()  # Render environment
            if done:  # If episode is done
                break
        print('Episode {}\tReward: {}'.format(i_episode, running_reward))  # Print episode reward
    env.close()  # Close environment


if __name__ == '__main__':
    # For reproducibility, fix the random seed
    random_seed = 10  # Random seed for reproducibility
    lr = 0.001  # Learning rate
    env = gym.make('CartPole-v0')  # Create CartPole environment
    env.seed(random_seed)  # Set random seed for environment
    torch.manual_seed(random_seed)  # Set random seed for PyTorch
    train(lr)  # Train the model
    test(f'CartPole_{lr}.pth')  # Test the model

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=./tb_record_1/Vanilla


# Import necessary libraries
import os  # Operating system library for file operations
import gym  # OpenAI Gym library for RL environments
from itertools import count  # Iterator tools for counting
from collections import namedtuple  # Collections library for named tuples
import numpy as np  # Numerical computing library

import torch  # PyTorch deep learning library
import torch.nn as nn  # Neural network module in PyTorch
import torch.nn.functional as F  # Functional module in PyTorch
import torch.optim as optim  # Optimization module in PyTorch
from torch.distributions import Categorical  # Probability distributions in PyTorch
import torch.optim.lr_scheduler as Scheduler  # Learning rate scheduler in PyTorch
from torch.utils.tensorboard import SummaryWriter  # TensorBoard integration for PyTorch

# Define a useful tuple (optional)
SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])

# Define a tensorboard writer
writer = SummaryWriter("./tb_record_1/Vanilla")  # Summary writer for TensorBoard

class Policy(nn.Module):
    """
        Implement both policy network and the value network in one model
        - Note that here we let the actor and value networks share the first layer
        - Feel free to change the architecture (e.g. number of hidden layers and the width of each hidden layer) as you like
        - Feel free to add any member variables/functions whenever needed
        TODO:
            1. Initialize the network (including the GAE parameters, shared layer(s), the action layer(s), and the value layer(s))
            2. Random weight initialization of each layer
    """
    def __init__(self):
        super(Policy, self).__init__()

        # Extract the dimensionality of state and action spaces
        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)  # Check if action space is discrete
        self.observation_dim = env.observation_space.shape[0]  # Dimensionality of the observation space
        self.action_dim = env.action_space.n if self.discrete else env.action_space.shape[0]  # Dimensionality of the action space
        self.hidden_size = 128  # Size of the hidden layer
        self.double()  # Convert to double precision floating point

        # Use CUDA if available
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        ########## YOUR CODE HERE (5~10 lines) ##########
        # Define neural network layers
        self.shared_layer1 = nn.Linear(self.observation_dim, self.hidden_size).to(self.device)  # Shared layer 1
        self.dropout = nn.Dropout(p=0.3)  # Dropout layer
        self.shared_layer2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)  # Shared layer 2
        self.action_layer = nn.Linear(self.hidden_size, self.action_dim).to(self.device)  # Action layer
        self.value_layer = nn.Linear(self.hidden_size, 1).to(self.device)  # Value layer
        ########## END OF YOUR CODE ##########

        # Action & reward memory
        self.saved_actions = []  # Stores the log probabilities and values of selected actions
        self.rewards = []  # Stores rewards

    def forward(self, state):
        """
            Forward pass of both policy and value networks
            - The input is the state, and the outputs are the corresponding
              action probability distribution and the state value
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        x = self.shared_layer1(state)  # Forward pass through shared layer 1
        x = F.relu(x)  # Apply ReLU activation function
        x = self.dropout(x)
        x = self.shared_layer2(x)  # Forward pass through shared layer 2
        x = F.relu(x)  # Apply ReLU activation function
        action_prob = self.action_layer(x)  # Action probabilities
        state_value = self.value_layer(x)  # State value estimation
        ########## END OF YOUR CODE ##########

        return action_prob, state_value

    def select_action(self, state):
        """
            Select the action given the current state
            - The input is the state, and the output is the action to apply
            (based on the learned stochastic policy)
            TODO:
                1. Implement the forward pass for both the action and the state value
        """

        ########## YOUR CODE HERE (3~5 lines) ##########
        state = torch.Tensor(state).to(self.device)  # Convert state to PyTorch tensor and move to device
        action, state_value = self.forward(state)  # Forward pass through the network
        m = Categorical(logits=action)  # Categorical distribution based on action probabilities
        action = m.sample()  # Sample an action from the distribution
        ########## END OF YOUR CODE ##########

        # Save action and log probability to action buffer
        self.saved_actions.append(SavedAction(m.log_prob(action), state_value))

        return action.item()  # Return the sampled action

    def calculate_loss(self, gamma=0.999):
        """
            Calculate the loss (= policy loss + value loss) to perform backprop later
            TODO:
                1. Calculate rewards-to-go required by REINFORCE with the help of self.rewards
                2. Calculate the policy loss using the policy gradient
                3. Calculate the value loss using either MSE loss or smooth L1 loss
        """

        # Initialize the lists and variables
        R = 0  # Initialize the rewards-to-go
        saved_actions = self.saved_actions  # Saved actions and log probabilities
        policy_losses = []  # Policy loss
        value_losses = []  # Value loss
        returns = []  # Returns (rewards-to-go)

        ########## YOUR CODE HERE (8-15 lines) ##########
        discounted_sum = 0  # Initialize discounted sum
        for reward in reversed(self.rewards):  # Iterate over rewards in reverse order
            discounted_sum = reward + gamma * discounted_sum  # Calculate discounted sum
            returns.append(discounted_sum)  # Append to returns
        returns.reverse()  # Reverse the list to align with chronological order

        returns = torch.Tensor(returns).to(self.device)  # Convert to PyTorch tensor and move to device
        log_probs = [action.log_prob for action in saved_actions]  # Log probabilities of actions
        action_log_probs = torch.stack(log_probs, dim=0)  # Stack log probabilities
        policy_losses = -(returns * action_log_probs).sum()  # Policy loss calculation
        ########## END OF YOUR CODE ##########

        return policy_losses  # Return the policy loss

    def clear_memory(self):
        # Reset rewards and action buffer
        del self.rewards[:]  # Clear rewards
        del self.saved_actions[:]  # Clear saved actions

class GAE:
    def __init__(self, gamma, lambda_, num_steps):
        self.gamma = gamma  # Discount factor
        self.lambda_ = lambda_  # Lambda parameter for GAE
        self.num_steps = num_steps  # Number of steps for GAE calculation
        # Set num_steps = None to adapt full batch

    def __call__(self, rewards, values, done):
        pass
        """
            Implement Generalized Advantage Estimation (GAE) for your value prediction
            TODO (1): Pass correct corresponding inputs (rewards, values, and done) into the function arguments
            TODO (2): Calculate the Generalized Advantage Estimation and return the obtained value
        """

        ########## YOUR CODE HERE (8-15 lines) ##########




        ########## END OF YOUR CODE ##########

def train(lr=0.01):
    """
        Train the model using SGD (via backpropagation)
        TODO (1): In each episode,
        1. run the policy till the end of the episode and keep the sampled trajectory
        2. update both the policy and the value network at the end of episode

        TODO (2): In each episode,
        1. record all the value you aim to visualize on tensorboard (lr, reward, length, ...)
    """

    # Instantiate the policy model and the optimizer
    model = Policy()  # Create policy model
    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer

    # Learning rate scheduler (optional)
    scheduler = Scheduler.StepLR(optimizer, step_size=100, gamma=0.9)

    # EWMA reward for tracking the learning progress
    ewma_reward = 0  # Exponentially Weighted Moving Average reward

    # Run infinitely many episodes
    for i_episode in count(1):
        # Reset environment and episode reward
        state = env.reset()  # Reset environment
        ep_reward = 0  # Initialize episode reward
        t = 0  # Initialize time step

        # Uncomment the following line to use learning rate scheduler
        scheduler.step()

        # For each episode, only run 9999 steps to avoid entering infinite loop during the learning process

        ########## YOUR CODE HERE (10-15 lines) ##########
        while True:
            t += 1  # Increment time step
            action = model.select_action(state)  # Select action based on current state
            state, reward, done, _ = env.step(action)  # Take action in the environment

            ep_reward += reward  # Update episode reward
            model.rewards.append(reward)  # Append reward to rewards buffer

            if done:  # If episode is done
                break

        optimizer.zero_grad()  # Zero gradients
        loss = model.calculate_loss()  # Calculate loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Optimizer step
        scheduler.step()  # Learning rate scheduler step

        model.clear_memory()  # Clear memory
        ########## END OF YOUR CODE ##########

        # Update EWMA reward and log the results
        ewma_reward = 0.05 * ep_reward + (1 - 0.05) * ewma_reward  # Update EWMA reward
        print('Episode {}\tlength: {}\treward: {}\t ewma reward: {}'.format(i_episode, t, ep_reward, ewma_reward))

        # Try to use Tensorboard to record the behavior of your implementation
        ########## YOUR CODE HERE (4-5 lines) ##########
        writer.add_scalar('training loss', loss, i_episode)  # Record training loss
        writer.add_scalar('EWMA reward', ewma_reward, i_episode)  # Record EWMA reward
        writer.add_scalar('Episode reward', ep_reward, i_episode)  # Record episode reward
        writer.add_scalar('Length', t, i_episode)  # Record episode length
        #writer.add_scalar('Learning Rate', scheduler.get_lr()[-1], i_episode)  # Record learning rate
        writer.add_scalar('Learning Rate', scheduler.get_last_lr()[-1], i_episode)  # Record learning rate get_last_lr()
        ########## END OF YOUR CODE ##########

        # Check if we have "solved" the cart pole problem, use 120 as the threshold in LunarLander-v2
        if ewma_reward > env.spec.reward_threshold:
            if not os.path.isdir("./preTrained"):  # If directory does not exist
                os.mkdir("./preTrained")  # Create directory
            torch.save(model.state_dict(), './preTrained/CartPole_{}.pth'.format(lr))  # Save model state
            print("Solved! Running reward is now {} and "
                  "the last episode runs to {} time steps!".format(ewma_reward, t))  # Print message
            break  # Break out of loop


def test(name, n_episodes=10):
    """
        Test the learned model (no change needed)
    """
    model = Policy()  # Create policy model

    model.load_state_dict(torch.load('./preTrained/{}'.format(name)))  # Load model parameters
    #model.to(torch.device("cuda"))  # Move model to CUDA

    render = True  # Rendering flag
    max_episode_len = 10000  # Maximum episode length

    for i_episode in range(1, n_episodes+1):
        state = env.reset()  # Reset environment
        running_reward = 0  # Initialize running reward
        for t in range(max_episode_len+1):
            action = model.select_action(state)  # Select action based on current state
            state, reward, done, _ = env.step(action)  # Take action in the environment
            running_reward += reward  # Update running reward
            if render:
                 env.render()  # Render environment
            if done:  # If episode is done
                break
        print('Episode {}\tReward: {}'.format(i_episode, running_reward))  # Print episode reward
    env.close()  # Close environment


if __name__ == '__main__':
    # For reproducibility, fix the random seed
    random_seed = 10  # Random seed for reproducibility
    lr = 0.001  # Learning rate
    env = gym.make('CartPole-v0')  # Create CartPole environment
    env.seed(random_seed)  # Set random seed for environment
    torch.manual_seed(random_seed)  # Set random seed for PyTorch
    train(lr)  # Train the model
    test(f'CartPole_{lr}.pth')  # Test the model

