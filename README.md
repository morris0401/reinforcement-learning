# reinforcement-learning

![language](https://img.shields.io/badge/language-Python-blue.svg)

Course materials for **reinforcement learning**.

## ðŸ“‚ Course Assignments

- **[Final Project](Final Project)**: Offline Reinforcement Learning (RL) presents a compelling paradigm for training intelligent agents from pre-collected datasets without costly and potentially unsafe environmental interactions. A fundamental challenge in this setting is the "extrapolation error" or out-of-distribution (OOD) state-action pairs. When a learned policy deviates from the data distribution it was trained on, Q-value estimations become unreliable, leading to suboptimal or catastrophic behavior. Batch-Constrained Q-learning (BCQ) addresses this by explicitly restricting the learned policy to actions likely to be found within the dataset. BCQ achieves this through a combination of a Variational Auto-Encoder (VAE) to generate in-batch actions, a perturbation model to diversify these actions, and Clipped Double Q-learning for robust value estimation. This project conducts an extensive ablation study on BCQ, exploring six distinct architectural and hyperparameter modifications to enhance its performance and better mitigate the inherent challenges of OOD state-action pairs across diverse D4RL benchmarks.
- **[HW1](HW1)**: Reinforcement Learning (RL) presents significant challenges in optimizing sequential decision-making, necessitating a robust understanding of its theoretical underpinnings and practical algorithms. This project delves into the core principles of RL, spanning fundamental theoretical proofs, the design and implementation of classical dynamic programming algorithms, and the practical exploration of real-world offline RL datasets. Our aim is to synthesize foundational conceptsâ€”from Bellman optimality and contraction mappings to policy gradient mechanicsâ€”with hands-on experience, bridging the gap between mathematical theory and its application in contemporary RL research, particularly within the offline learning paradigm.
- **[HW2](HW2)**: Policy gradient methods are foundational in reinforcement learning (RL), enabling agents to learn complex behaviors without requiring an explicit model of the environment. However, a significant challenge in applying these methods, especially with deep neural network function approximators, is the high variance in gradient estimates, which can lead to unstable training and slow convergence. Furthermore, understanding the theoretical convergence properties of these algorithms in non-convex settings remains an active area of research. This project addresses these critical issues by integrating theoretical analysis of policy optimization with rigorous empirical investigation into variance reduction techniques and neural network configurations. Our goal is to enhance the stability, efficiency, and theoretical understanding of policy gradient algorithms, paving the way for more robust deep RL applications.
- **[HW3](HW3)**: Achieving robust and efficient control in environments with continuous action spaces remains a fundamental challenge in Reinforcement Learning (RL). Policy gradient methods, especially those incorporating stability mechanisms, have shown significant promise. This work explores the theoretical underpinnings and practical applications of advanced policy optimization algorithms, specifically Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), and Deep Deterministic Policy Gradient (DDPG). Our objective is to deepen the understanding of their operational principles, including their constrained optimization landscapes and update mechanisms, and to demonstrate their efficacy in solving complex continuous control tasks through deep neural network implementations.
